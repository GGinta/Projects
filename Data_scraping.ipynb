{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c32d58",
   "metadata": {},
   "source": [
    "***\n",
    "<div  align='center'><img src='https://cdn-res.keymedia.com/cms/images/ca/155/0348_637304012816162034.jpg' width='25%'></div >\n",
    "<h1><center> WebScraping project: Ottawa Housing Market </center></h1>\n",
    "<h2><center>Created by: Ginta Grinfelde</center></h2> \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0c686",
   "metadata": {},
   "source": [
    "## Importing  libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa88fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import requests # Website connections\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "import json # For parsing json\n",
    "%matplotlib inline \n",
    "import re #For searching text\n",
    "from random import randint # for sleep randomisation\n",
    "import math # using math formulas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac3503e",
   "metadata": {},
   "source": [
    "## 1. Testing if the website allows webscraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2017a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url = 'https://www.point2homes.com/CA/Real-Estate-Listings/ON/Ottawa.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2cf10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c15b81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.status_code # Good status! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd8794",
   "metadata": {},
   "source": [
    "## 2. Getting urls for each listing by using Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a3957a",
   "metadata": {},
   "source": [
    "### Getting content from the first page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb24b4",
   "metadata": {},
   "source": [
    "After some failed attempts, I found out that the website only displays 30 pages of listings (24 per page). I had to use a different approach and use filters to get multiple search results. I added 5 different price ranges and scraped the pages for each search result. In this way, I was able to scrape 25-30 pages per price break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9813882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining get_page function and getting the content from the first page - Real Estate Listings in Ottawa, Ontario.\n",
    "# The same code can be used if using other provinces and cities.\n",
    "\n",
    "def get_page(province, city, page, price_min, price_max):\n",
    "    url = f'https://www.point2homes.com/CA/Real-Estate-Listings/{province}/{city}.html?page={page}&PriceMin={price_min}&PriceMax={price_max}'\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.content)\n",
    "    return soup\n",
    "soup = get_page('ON', 'Ottawa', 1, 0, 250000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f17a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting price ranges to set search filters and get more pages to scrap\n",
    "price_range = [\n",
    "    ['0', '500000'],\n",
    "    ['500001', '700000'],\n",
    "    ['700001', '1000000'],\n",
    "    ['1000000', '2000000'],\n",
    "    ['2000001', '4000000']]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32187a7d",
   "metadata": {},
   "source": [
    "### Getting urls for each listing from all pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97afb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing sleep to slow down the code\n",
    "from time import sleep\n",
    "list_urls = []\n",
    "sleep = sleep(randint(1,10))\n",
    "\n",
    "# Getting the number of search results per price filter(from the first pages) and the number of listings per page and\n",
    "# Calculating the amount of pages to scrape for each price break.\n",
    "\n",
    "for x in price_range:\n",
    "    low = x[0]\n",
    "    high = x[1]\n",
    "    soup = get_page('ON', 'Ottawa', 1, low, high)\n",
    "    num_results_per_page = int(soup.find('div', class_='results-no').get_text(strip=True).split()[2].replace(',', ''))\n",
    "    num_of_results = int(soup.find('div', class_='results-no').get_text(strip=True).split()[-2].replace(',', ''))\n",
    "    num_of_pages = math.ceil(num_of_results/num_results_per_page)\n",
    "\n",
    "    #Setting up the function to scrape all pages    \n",
    "    for page in range(1,num_of_pages+1):\n",
    "        sleep\n",
    "        soup = get_page('ON', 'Ottawa', page, low, high)\n",
    "    \n",
    "    #Getting the urls for each listing from each page and storing them in list_urls list\n",
    "        for addr in soup.find_all('h3', class_='item-address-title'):\n",
    "            sleep\n",
    "            list_urls.append(addr.a.get('href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ed4ba",
   "metadata": {},
   "source": [
    "## 3. Collecting data from each listing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa811d2",
   "metadata": {},
   "source": [
    "### Determining which attributes to scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabfb041",
   "metadata": {},
   "source": [
    "To have a better understanding of the available properties, I decided to scrape the following information from each listing:\n",
    "\n",
    "0. prop_address = Property address\n",
    "1. price = Listed selling price\n",
    "2. num_beds = Number of bedrooms\n",
    "3. num_baths - Number of bathrooms\n",
    "4. prop_type = Property type (Residential, Single family etc)\n",
    "5. prop_style = Property style (Semi-detached, Condominium etc)\n",
    "6. year_built = When the house was built\n",
    "7. parking = Available parking  at the property\n",
    "8. basement = If the roperty has a basement\n",
    "9. prop_taxes = Estimated yearly taxes for houses\n",
    "10. neighborhood = Property's neighborhood\n",
    "11. postal_code = Property's postal_code\n",
    "12. lot_info = Size of the property\n",
    "13. assoc_fee = Estimated fees for condominiums\n",
    "14. walk_score = Walk Score is a number between 0 and 100 that measures the walkability of the address.\n",
    "15. transit_score = Transit Score is a number between 0 and 100 that shows how close the public transport is to the address.\n",
    "16. bike_score = Bike Score is a number between 0 and 100 that measures the bikability of the address.\n",
    "17. latitude = addresse's latitude for using maps\n",
    "18. longitude = addresse's longitude for using maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43bc51a",
   "metadata": {},
   "source": [
    "### Defining the full urls for each listing's page and getting the content.\n",
    "The urls in list_urls are partial so a new functions has to be defined for scraping each listing's page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b11979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The link changes from the previous get_page function so new function is created.\n",
    "# During data scraping I found out that not all urls are partial so I added an if statement to take care of it.\n",
    "\n",
    "def get_page2(ext):\n",
    "    if 'http' in ext:\n",
    "        url = ext\n",
    "    else:\n",
    "        url = f'https://www.point2homes.com{ext}'\n",
    "    result = requests.get(url)\n",
    "    p_content = BeautifulSoup(result.content)\n",
    "    return p_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482bbac4",
   "metadata": {},
   "source": [
    "### Scraping informations based on above atrributes from each property listing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and adding the sleep function again (it kept breaking for me)\n",
    "from time import sleep\n",
    "sleep = sleep(randint(1,10))\n",
    "\n",
    "# Getting data from each listing and storing it in a list of lists - data_p\n",
    "\n",
    "data_p = []\n",
    "\n",
    "for url in list_urls:\n",
    "    sleep\n",
    "    p_content = get_page2(url)\n",
    "    \n",
    "    for tag in p_content:\n",
    "        sleep \n",
    "        \n",
    "  #For each attribute I added a Misssing Info option if the listing doesn't have the attribute.\n",
    "\n",
    "        #0\n",
    "        try:\n",
    "            prop_address = p_content.find('div', class_='address-container').get_text(strip=True)\n",
    "        except:\n",
    "            prop_address = 'Missing info'\n",
    "        #1\n",
    "        sleep \n",
    "        try:\n",
    "            price = p_content.find('div', class_='price').get_text(strip=True)\n",
    "        except:\n",
    "            price = 'Missing info'\n",
    "\n",
    "        #2\n",
    "        sleep \n",
    "        try:\n",
    "            num_beds = p_content.find('li', class_='ic-beds').get_text(strip=True)[0]\n",
    "        except:\n",
    "            num_beds = 'Missing info'\n",
    "\n",
    "        #3\n",
    "        sleep \n",
    "        try:\n",
    "            num_baths = p_content.find('li', class_='ic-baths nosq').get_text(strip=True)[0]\n",
    "        except:\n",
    "            num_baths = 'Missing info'\n",
    "        sleep \n",
    "        #4\n",
    "        try:\n",
    "            prop_type = p_content.find('li', class_='property-type ic-proptype').get_text(strip=True)\n",
    "        except:\n",
    "            prop_type = 'Missing info'\n",
    "        sleep \n",
    "         #5   \n",
    "        try:\n",
    "            prop_style = p_content.find('dt', text='Style' ).find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            prop_style = 'Missing info'\n",
    "        sleep \n",
    "        #6\n",
    "        try:\n",
    "            year_built = p_content.find('dt', text='Year Built' ).find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            year_built = 'Missing info'\n",
    "        sleep \n",
    "        #7   \n",
    "        try:\n",
    "            parking = p_content.find('dt', text='Parking info').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            parking = 'Missing info'\n",
    "        sleep \n",
    "        #8    \n",
    "        try:\n",
    "            basement = p_content.find('dt', text='Basement').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            basement = 'Missing info'\n",
    "        sleep\n",
    "        #9    \n",
    "        try:\n",
    "            prop_taxes = p_content.find('dt', text='Taxes').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            prop_taxes = 'Missing info'\n",
    "        sleep \n",
    "        #10\n",
    "        try:\n",
    "            neighborhood = p_content.find('dt', text='Neighborhood').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            neighborhood = 'Missing info'\n",
    "        sleep \n",
    "        #11\n",
    "        try:\n",
    "            postal_code = p_content.find('dt', text='Postal Code').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            postal_code = 'Missing info'\n",
    "        sleep \n",
    "        #12\n",
    "        try:\n",
    "            lot_info = p_content.find('dt', text='Lot info').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            lot_info = 'Missing info'\n",
    "        sleep \n",
    "        #13\n",
    "        try:\n",
    "            assoc_fee = p_content.find('dt', text='Association Fee').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            assoc_fee = 'Missing info'\n",
    "        sleep \n",
    "        #14   \n",
    "        try:\n",
    "            walk_score = list(p_content.find('div', class_='walkscore-item walkscore-ic1').span.children)[0].get_text(strip=True)\n",
    "        except:\n",
    "            walk_score = 'Missing info'\n",
    "        sleep \n",
    "        #15\n",
    "        try:\n",
    "            transit_score = list(p_content.find('div', class_='walkscore-item walkscore-ic2').span.children)[0].get_text(strip=True)\n",
    "        except:\n",
    "            transit_score = 'Missing info'\n",
    "        sleep \n",
    "         #16   \n",
    "        try:\n",
    "            bike_score = list(p_content.find('div', class_='walkscore-item walkscore-ic3').span.children)[0].get_text(strip=True)\n",
    "        except:\n",
    "            bike_score = 'Missing info'\n",
    "        sleep \n",
    "        #17    \n",
    "        try:\n",
    "            latitude = p_content.find('input', id=(re.findall('Latitude\\_+.\\_[\\d]+', str(p_content))[0]))['value']\n",
    "        except:\n",
    "            latitude = 'Missing info'\n",
    "        sleep \n",
    "        #18\n",
    "        try:\n",
    "            longitude= p_content.find('input', id=(re.findall('Longitude\\_+.\\_[\\d]+', str(p_content))[0]))['value']\n",
    "        except:\n",
    "            longitude = 'Missing info'\n",
    "\n",
    "    \n",
    "        \n",
    "    data_p.append([prop_address, price, num_beds, num_baths, prop_type\\\n",
    "             ,prop_style, year_built, parking, basement\\\n",
    "             ,prop_taxes, neighborhood, postal_code, lot_info, assoc_fee\\\n",
    "             ,walk_score, transit_score, bike_score, latitude, longitude])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d11e59",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec40fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go \n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8155f9",
   "metadata": {},
   "source": [
    "Creating a new DataFrame 'properties' from a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deec505",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = pd.DataFrame(data_p, columns= ['prop_address', 'price', 'num_beds', 'num_baths', 'prop_type'\\\n",
    "             ,'prop_style', 'year_built', 'parking', 'basement'\\\n",
    "             ,'prop_taxes', 'neighborhood', 'postal_code', 'lot_info', 'assoc_fee'\\\n",
    "             ,'walk_score', 'transit_score', 'bike_score', 'latitude', 'longitude']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89793c",
   "metadata": {},
   "source": [
    "### Exporting the data to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25150e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties.to_csv('/Users/gintagrinfelde/Documents/Data Science/Python/DataScraping_python-Ginta/point2homes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c25180",
   "metadata": {},
   "source": [
    "#### The scraped data Exploration can be found in the Data Analysis notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
