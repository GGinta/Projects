{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c32d58",
   "metadata": {},
   "source": [
    "***\n",
    "<div  align='center'><img src='https://cdn-res.keymedia.com/cms/images/ca/155/0348_637304012816162034.jpg' width='25%'></div >\n",
    "<h1><center> WebScraping project: Ottawa Housing Market </center></h1>\n",
    "<h2><center>Created by: Ginta Grinfelde</center></h2> \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0c686",
   "metadata": {},
   "source": [
    "## Importing  libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa88fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import requests # Website connections\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "import json # For parsing json\n",
    "%matplotlib inline \n",
    "import re #For searching text\n",
    "from random import randint # for sleep randomisation\n",
    "import math # using math formulas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac3503e",
   "metadata": {},
   "source": [
    "## 1. Testing if the website allows webscraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2017a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url = 'https://www.point2homes.com/CA/Real-Estate-Listings/ON/Ottawa.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2cf10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c15b81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.status_code # Good status! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd8794",
   "metadata": {},
   "source": [
    "## 2. Getting urls for each listing by using Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a3957a",
   "metadata": {},
   "source": [
    "### Getting content from the first page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb24b4",
   "metadata": {},
   "source": [
    "After some failed attempts, I found out that the website only displays 30 pages of listings (24 per page). I had to use a different approach and use filters to get multiple search results. I added 5 different price ranges and scraped the pages for each search result. In this way, I was able to scrape 25-30 pages per price break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9813882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining get_page function and getting the content from the first page - Real Estate Listings in Ottawa, Ontario.\n",
    "# The same code can be used if using other provinces and cities.\n",
    "\n",
    "def get_page(province, city, page, price_min, price_max):\n",
    "    url = f'https://www.point2homes.com/CA/Real-Estate-Listings/{province}/{city}.html?page={page}&PriceMin={price_min}&PriceMax={price_max}'\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.content)\n",
    "    return soup\n",
    "soup = get_page('ON', 'Ottawa', 1, 0, 250000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f17a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting price ranges to set search filters and get more pages to scrap\n",
    "price_range = [\n",
    "    ['0', '500000'],\n",
    "    ['500001', '700000'],\n",
    "    ['700001', '1000000'],\n",
    "    ['1000000', '2000000'],\n",
    "    ['2000001', '4000000']]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32187a7d",
   "metadata": {},
   "source": [
    "### Getting urls for each listing from all pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e017245a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_of_pages\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     19\u001b[0m     sleep\n\u001b[0;32m---> 20\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mON\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOttawa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhigh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#Getting the urls for each listing from each page and storing them in list_urls list\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m addr \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem-address-title\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mget_page\u001b[0;34m(province, city, page, price_min, price_max)\u001b[0m\n\u001b[1;32m      5\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.point2homes.com/CA/Real-Estate-Listings/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovince\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html?page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&PriceMin=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprice_min\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&PriceMax=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprice_max\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m result \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m----> 7\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m soup\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:333\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:451\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/bs4/builder/_lxml.py:378\u001b[0m, in \u001b[0;36mLXMLTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser_for(encoding)\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mLookupError\u001b[39;00m, etree\u001b[38;5;241m.\u001b[39mParserError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32msrc/lxml/parser.pxi:1256\u001b[0m, in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parser.pxi:1376\u001b[0m, in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parsertarget.pxi:168\u001b[0m, in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parsertarget.pxi:156\u001b[0m, in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/etree.pyx:333\u001b[0m, in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/saxparser.pxi:443\u001b[0m, in \u001b[0;36mlxml.etree._handleSaxTargetStartNoNs\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/saxparser.pxi:458\u001b[0m, in \u001b[0;36mlxml.etree._callTargetSaxStart\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parsertarget.pxi:94\u001b[0m, in \u001b[0;36mlxml.etree._PythonSaxParserTarget._handleSaxStart\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/bs4/builder/_lxml.py:301\u001b[0m, in \u001b[0;36mLXMLTreeBuilderForXML.start\u001b[0;34m(self, name, attrs, nsmap)\u001b[0m\n\u001b[1;32m    299\u001b[0m namespace, name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getNsTag(name)\n\u001b[1;32m    300\u001b[0m nsprefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefix_for_namespace(namespace)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_namespace_prefixes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:732\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_starttag\u001b[0;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_most_recent_element\u001b[38;5;241m.\u001b[39mnext_element \u001b[38;5;241m=\u001b[39m tag\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_most_recent_element \u001b[38;5;241m=\u001b[39m tag\n\u001b[0;32m--> 732\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpushTag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tag\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:552\u001b[0m, in \u001b[0;36mBeautifulSoup.pushTag\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentTag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagStack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mROOT_TAG_NAME:\n\u001b[0;32m--> 552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen_tag_counter[tag\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mpreserve_whitespace_tags:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreserve_whitespace_tag_stack\u001b[38;5;241m.\u001b[39mappend(tag)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Importing sleep to slow down the code\n",
    "from time import sleep\n",
    "list_urls = []\n",
    "sleep = sleep(randint(1,10))\n",
    "\n",
    "# Getting the number of search results per price filter(from the first pages) and the number of listings per page and\n",
    "# Calculating the amount of pages to scrape for each price break.\n",
    "\n",
    "for x in price_range:\n",
    "    low = x[0]\n",
    "    high = x[1]\n",
    "    soup = get_page('ON', 'Ottawa', 1, low, high)\n",
    "    num_results_per_page = int(soup.find('div', class_='results-no').get_text(strip=True).split()[2].replace(',', ''))\n",
    "    num_of_results = int(soup.find('div', class_='results-no').get_text(strip=True).split()[-2].replace(',', ''))\n",
    "    num_of_pages = math.ceil(num_of_results/num_results_per_page)\n",
    "\n",
    "    #Setting up the function to scrape all pages    \n",
    "    for page in range(1,num_of_pages+1):\n",
    "        sleep\n",
    "        soup = get_page('ON', 'Ottawa', page, low, high)\n",
    "    \n",
    "    #Getting the urls for each listing from each page and storing them in list_urls list\n",
    "        for addr in soup.find_all('h3', class_='item-address-title'):\n",
    "            sleep\n",
    "            list_urls.append(addr.a.get('href'))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ed4ba",
   "metadata": {},
   "source": [
    "## 3. Collecting data from each listing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa811d2",
   "metadata": {},
   "source": [
    "### Determining which attributes to scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabfb041",
   "metadata": {},
   "source": [
    "To have a better understanding of the available properties, I decided to scrape the following information from each listing:\n",
    "\n",
    "0. prop_address = Property address\n",
    "1. price = Listed selling price\n",
    "2. num_beds = Number of bedrooms\n",
    "3. num_baths - Number of bathrooms\n",
    "4. prop_type = Property type (Residential, Single family etc)\n",
    "5. prop_style = Property style (Semi-detached, Condominium etc)\n",
    "6. year_built = When the house was built\n",
    "7. parking = Available parking  at the property\n",
    "8. basement = If the roperty has a basement\n",
    "9. prop_taxes = Estimated yearly taxes for houses\n",
    "10. neighborhood = Property's neighborhood\n",
    "11. postal_code = Property's postal_code\n",
    "12. lot_info = Size of the property\n",
    "13. assoc_fee = Estimated fees for condominiums\n",
    "14. walk_score = Walk Score is a number between 0 and 100 that measures the walkability of the address.\n",
    "15. transit_score = Transit Score is a number between 0 and 100 that shows how close the public transport is to the address.\n",
    "16. bike_score = Bike Score is a number between 0 and 100 that measures the bikability of the address.\n",
    "17. latitude = addresse's latitude for using maps\n",
    "18. longitude = addresse's longitude for using maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43bc51a",
   "metadata": {},
   "source": [
    "### Defining the full urls for each listing's page and getting the content.\n",
    "The urls in list_urls are partial so a new functions has to be defined for scraping each listing's page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b11979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The link changes from the previous get_page function so new function is created.\n",
    "# During data scraping I found out that not all urls are partial so I added an if statement to take care of it.\n",
    "\n",
    "def get_page2(ext):\n",
    "    if 'http' in ext:\n",
    "        url = ext\n",
    "    else:\n",
    "        url = f'https://www.point2homes.com{ext}'\n",
    "    result = requests.get(url)\n",
    "    p_content = BeautifulSoup(result.content)\n",
    "    return p_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482bbac4",
   "metadata": {},
   "source": [
    "### Scraping informations based on above atrributes from each property listing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and adding the sleep function again (it kept breaking for me)\n",
    "from time import sleep\n",
    "sleep = sleep(randint(1,10))\n",
    "\n",
    "# Getting data from each listing and storing it in a list of lists - data_p\n",
    "\n",
    "data_p = []\n",
    "\n",
    "for url in list_urls:\n",
    "    sleep\n",
    "    p_content = get_page2(url)\n",
    "    \n",
    "    for tag in p_content:\n",
    "        sleep \n",
    "        \n",
    "  #For each attribute I added a Misssing Info option if the listing doesn't have the attribute.\n",
    "\n",
    "        #0\n",
    "        try:\n",
    "            prop_address = p_content.find('div', class_='address-container').get_text(strip=True)\n",
    "        except:\n",
    "            prop_address = 'Missing info'\n",
    "        #1\n",
    "        sleep \n",
    "        try:\n",
    "            price = p_content.find('div', class_='price').get_text(strip=True)\n",
    "        except:\n",
    "            price = 'Missing info'\n",
    "\n",
    "        #2\n",
    "        sleep \n",
    "        try:\n",
    "            num_beds = p_content.find('li', class_='ic-beds').get_text(strip=True)[0]\n",
    "        except:\n",
    "            num_beds = 'Missing info'\n",
    "\n",
    "        #3\n",
    "        sleep \n",
    "        try:\n",
    "            num_baths = p_content.find('li', class_='ic-baths nosq').get_text(strip=True)[0]\n",
    "        except:\n",
    "            num_baths = 'Missing info'\n",
    "        sleep \n",
    "        #4\n",
    "        try:\n",
    "            prop_type = p_content.find('li', class_='property-type ic-proptype').get_text(strip=True)\n",
    "        except:\n",
    "            prop_type = 'Missing info'\n",
    "        sleep \n",
    "         #5   \n",
    "        try:\n",
    "            prop_style = p_content.find('dt', text='Style' ).find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            prop_style = 'Missing info'\n",
    "        sleep \n",
    "        #6\n",
    "        try:\n",
    "            year_built = p_content.find('dt', text='Year Built' ).find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            year_built = 'Missing info'\n",
    "        sleep \n",
    "        #7   \n",
    "        try:\n",
    "            parking = p_content.find('dt', text='Parking info').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            parking = 'Missing info'\n",
    "        sleep \n",
    "        #8    \n",
    "        try:\n",
    "            basement = p_content.find('dt', text='Basement').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            basement = 'Missing info'\n",
    "        sleep\n",
    "        #9    \n",
    "        try:\n",
    "            prop_taxes = p_content.find('dt', text='Taxes').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            prop_taxes = 'Missing info'\n",
    "        sleep \n",
    "        #10\n",
    "        try:\n",
    "            neighborhood = p_content.find('dt', text='Neighborhood').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            neighborhood = 'Missing info'\n",
    "        sleep \n",
    "        #11\n",
    "        try:\n",
    "            postal_code = p_content.find('dt', text='Postal Code').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            postal_code = 'Missing info'\n",
    "        sleep \n",
    "        #12\n",
    "        try:\n",
    "            lot_info = p_content.find('dt', text='Lot info').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            lot_info = 'Missing info'\n",
    "        sleep \n",
    "        #13\n",
    "        try:\n",
    "            assoc_fee = p_content.find('dt', text='Association Fee').find_next_sibling(\"dd\").get_text(strip=True)\n",
    "        except:\n",
    "            assoc_fee = 'Missing info'\n",
    "        sleep \n",
    "        #14   \n",
    "        try:\n",
    "            walk_score = list(p_content.find('div', class_='walkscore-item walkscore-ic1').span.children)[0].get_text(strip=True)\n",
    "        except:\n",
    "            walk_score = 'Missing info'\n",
    "        sleep \n",
    "        #15\n",
    "        try:\n",
    "            transit_score = list(p_content.find('div', class_='walkscore-item walkscore-ic2').span.children)[0].get_text(strip=True)\n",
    "        except:\n",
    "            transit_score = 'Missing info'\n",
    "        sleep \n",
    "         #16   \n",
    "        try:\n",
    "            bike_score = list(p_content.find('div', class_='walkscore-item walkscore-ic3').span.children)[0].get_text(strip=True)\n",
    "        except:\n",
    "            bike_score = 'Missing info'\n",
    "        sleep \n",
    "        #17    \n",
    "        try:\n",
    "            latitude = p_content.find('input', id=(re.findall('Latitude\\_+.\\_[\\d]+', str(p_content))[0]))['value']\n",
    "        except:\n",
    "            latitude = 'Missing info'\n",
    "        sleep \n",
    "        #18\n",
    "        try:\n",
    "            longitude= p_content.find('input', id=(re.findall('Longitude\\_+.\\_[\\d]+', str(p_content))[0]))['value']\n",
    "        except:\n",
    "            longitude = 'Missing info'\n",
    "\n",
    "    \n",
    "        \n",
    "    data_p.append([prop_address, price, num_beds, num_baths, prop_type\\\n",
    "             ,prop_style, year_built, parking, basement\\\n",
    "             ,prop_taxes, neighborhood, postal_code, lot_info, assoc_fee\\\n",
    "             ,walk_score, transit_score, bike_score, latitude, longitude])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d11e59",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec40fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go \n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8155f9",
   "metadata": {},
   "source": [
    "Creating a new DataFrame 'properties' from a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe0ab99",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m properties \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdata_p\u001b[49m, columns\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprop_address\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_beds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_baths\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprop_type\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m      2\u001b[0m              ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprop_style\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_built\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparking\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasement\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m      3\u001b[0m              ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprop_taxes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneighborhood\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostal_code\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlot_info\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massoc_fee\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m      4\u001b[0m              ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwalk_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransit_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbike_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_p' is not defined"
     ]
    }
   ],
   "source": [
    "properties = pd.DataFrame(data_p, columns= ['prop_address', 'price', 'num_beds', 'num_baths', 'prop_type'\\\n",
    "             ,'prop_style', 'year_built', 'parking', 'basement'\\\n",
    "             ,'prop_taxes', 'neighborhood', 'postal_code', 'lot_info', 'assoc_fee'\\\n",
    "             ,'walk_score', 'transit_score', 'bike_score', 'latitude', 'longitude']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89793c",
   "metadata": {},
   "source": [
    "### Exporting the data to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e4bcdf4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'properties' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# A copy of the csv is provided in the zip file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mproperties\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/gintagrinfelde/Documents/Data Science/Python/DataScraping_python-Ginta/point2homes.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'properties' is not defined"
     ]
    }
   ],
   "source": [
    "properties.to_csv('/Users/gintagrinfelde/Documents/Data Science/Python/DataScraping_python-Ginta/point2homes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c25180",
   "metadata": {},
   "source": [
    "#### The scraped data Exploration can be found in the Data Analysis notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
